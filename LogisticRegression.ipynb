{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 从文件中读取词数据，并将词数据生成集合返回"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_word_set(words_file_path):\n",
    "    '''\n",
    "    从文件中读取词数据，并将词数据生成集合返回\n",
    "    :words_file_path 保存词数据的文件路径\n",
    "    :return 词集合\n",
    "    '''\n",
    "    words_set = set()\n",
    "    with open(words_file_path, 'r') as fp:\n",
    "        for line in fp.readlines():\n",
    "            word = line.strip().decode(\"utf-8\")\n",
    "            # 去掉重复的词\n",
    "            if len(word)>0 and word not in words_set:\n",
    "                words_set.add(word)\n",
    "    return words_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用jieba分词将句子切分，并将切分好的句子保存到文件中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cut_files(corpus_path, write_path, stopwords):\n",
    "    '''\n",
    "    切分文件中的句子，并把切好的句子保存到文件中\n",
    "    :corpus_path 未切词的原始句子输入文件的根目录路径\n",
    "    :write_path 切词后的句子的写入路径\n",
    "    '''\n",
    "    import jieba\n",
    "    # 闭包，切分单个句子\n",
    "    def cut_sentence(sentence, stopwords):\n",
    "        \"\"\"\n",
    "        闭包，使用jieba分词将句子切分\n",
    "        :sentence 要切分的句子\n",
    "        :stopwords 停用词集合\n",
    "        \"\"\"\n",
    "        sentence_cut = jieba.cut(sentence.strip())\n",
    "        # 最终以字符串形式返回\n",
    "        result = ''\n",
    "        for word in sentence_cut:\n",
    "            word = word.strip()\n",
    "            if word not in stopwords:\n",
    "                if len(word) > 0:\n",
    "                    result += word\n",
    "                    result += \" \"\n",
    "        return result\n",
    "\n",
    "    # 打开写入的文件\n",
    "    with open(write_path, 'w+') as w:\n",
    "        # 遍历并打开要读取的文件\n",
    "        import os\n",
    "        class_dir_names = os.listdir(corpus_path)\n",
    "        for class_dir_name in class_dir_names:\n",
    "            file_names = os.listdir(corpus_path + '/' + class_dir_name)\n",
    "            for file_name in file_names:\n",
    "                with open(corpus_path + '/' + class_dir_name + '/' + file_name) as f:\n",
    "                    # 去除内容两侧空白符，替换换行符、制表符为空格，并编码为utf-8\n",
    "                    content = f.read().decode('utf-8').strip().replace('\\n', ' ').replace('\\t', ' ')\n",
    "                    # 切分句子\n",
    "                    token_sen = cut_sentence(content, stopwords)\n",
    "                    # 将切分好的句子打上标签写入文件\n",
    "                    w.write(class_dir_name.encode('utf-8') + '\\t' + token_sen.encode('utf-8') + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 从原始文件中读取文本，进行预处理并打上标签（仅在需要的时候执行）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4min 38s, sys: 1.17 s, total: 4min 39s\n",
      "Wall time: 4min 39s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "stopwords = make_word_set('/home/beanyon/Desktop/logistic_regression/tools/tokenizer/stopwords.txt')\n",
    "cut_files('/home/beanyon/Desktop/logistic_regression/Sample', '/home/beanyon/Desktop/logistic_regression/corpus.txt', stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 从文件中获取并组装样本-13个类别各6500个文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_samples_from_file_6500(file_path):\n",
    "    \"\"\"\n",
    "    从文件中获取并组装样本\n",
    "    :file_path 存放样本的文件路径\n",
    "    :return 标签列表,语料列表\n",
    "    \"\"\"\n",
    "    label_list = []\n",
    "    corpus_list = []\n",
    "    with open(file_path, 'r') as p:\n",
    "        for line in p.readlines():\n",
    "            sample = line.split('\\t')\n",
    "            label_list.append(sample[0])\n",
    "            corpus_list.append(sample[1])\n",
    "    \n",
    "    # 将样本乱序\n",
    "    sample_list = zip(label_list, corpus_list)\n",
    "    import random\n",
    "    random.shuffle(sample_list)\n",
    "    label_list, corpus_list = zip(*sample_list)\n",
    "    \n",
    "    # 分割训练集、验证集、测试集\n",
    "    train_label = label_list[:65000]\n",
    "    val_label = label_list[65000:71500]\n",
    "    test_label = label_list[71500:]\n",
    "    train_corpus = corpus_list[:65000]\n",
    "    val_corpus = corpus_list[65000:71500]\n",
    "    test_corpus = corpus_list[71500:]\n",
    "    \n",
    "    return train_label, train_corpus, val_label, val_corpus, test_label, test_corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 从文件中获取并组装样本-11个类别各20000个文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_samples_from_file_20000(file_path):\n",
    "    \"\"\"\n",
    "    从文件中获取并组装样本\n",
    "    :file_path 存放样本的文件路径\n",
    "    :return 标签列表,语料列表\n",
    "    \"\"\"\n",
    "    label_list = []\n",
    "    corpus_list = []\n",
    "    with open(file_path, 'r') as p:\n",
    "        for line in p.readlines():\n",
    "            sample = line.split('\\t')\n",
    "            label_list.append(sample[0])\n",
    "            corpus_list.append(sample[1])\n",
    "    \n",
    "    # 将样本乱序\n",
    "    sample_list = zip(label_list, corpus_list)\n",
    "    import random\n",
    "    random.shuffle(sample_list)\n",
    "    label_list, corpus_list = zip(*sample_list)\n",
    "    \n",
    "    # 分割训练集、验证集、测试集\n",
    "    train_label = label_list[:165000]\n",
    "    val_label = label_list[165000:176000]\n",
    "    test_label = label_list[176000:]\n",
    "    train_corpus = corpus_list[:165000]\n",
    "    val_corpus = corpus_list[165000:176000]\n",
    "    test_corpus = corpus_list[176000:]\n",
    "    \n",
    "    return train_label, train_corpus, val_label, val_corpus, test_label, test_corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 将训练集、验证集、测试集合并，获取全部样本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_corpus(train_set, val_set, test_set):\n",
    "    '''\n",
    "    将训练集、验证集、测试集合并，获取全部样本\n",
    "    :train_set 训练集样本列表\n",
    "    :val_set 验证集样本列表\n",
    "    :test_set 测试集样本列表\n",
    "    :return 合并后的全体样本列表\n",
    "    '''\n",
    "    return train_set + val_set + test_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 计算TF-IDF值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_tf_idf(corpus_set):\n",
    "    '''\n",
    "    计算TF-IDF值\n",
    "    :train_corpus 训练集语料库\n",
    "    :return TF-IDF值集合\n",
    "    '''\n",
    "    from sklearn.feature_extraction.text import TfidfTransformer\n",
    "    from sklearn.feature_extraction.text import CountVectorizer\n",
    "    # drop df < 1e-5,去低频词\n",
    "    vectorizer = CountVectorizer(min_df=1e-5)\n",
    "    transformer = TfidfTransformer()\n",
    "    vectors = vectorizer.fit_transform(corpus_set)\n",
    "    tf_idf = transformer.fit_transform(vectors)\n",
    "    words = vectorizer.get_feature_names()\n",
    "    print \"how many words: {0}\".format(len(words))\n",
    "    print \"tf-idf shape: ({0},{1})\".format(tf_idf.shape[0], tf_idf.shape[1])\n",
    "    print \"vectors shape: ({0},{1})\".format(vectors.shape[0], vectors.shape[1])\n",
    "    return tf_idf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用逻辑回归进行分类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_classifier(train_set_features, train_label, \n",
    "                        val_set_features, val_label, \n",
    "                        test_set_features, test_label):\n",
    "    '''\n",
    "    使用逻辑回归进行分类\n",
    "    :train_set_features 训练集特征\n",
    "    :train_label 训练集标签\n",
    "    :val_set_features 验证集特征\n",
    "    :val_label 验证集标签\n",
    "    :test_set_features 测试集特征\n",
    "    :test_label 测试集标签\n",
    "    '''\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from sklearn.metrics import classification_report\n",
    "    \n",
    "    lr_model = LogisticRegression() \n",
    "    lr_model.fit(train_set_features, train_label) \n",
    "    print \"val mean accuracy: {0}\".format(lr_model.score(val_set_features, val_label))\n",
    "    y_pred = lr_model.predict(test_set_features)\n",
    "    print classification_report(test_label, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用随机森林进行分类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_forest_classifier(train_set_features, train_label, \n",
    "                             val_set_features, val_label, \n",
    "                             test_set_features, test_label, \n",
    "                             n_est=200):\n",
    "    '''\n",
    "    使用随机森林进行分类\n",
    "    :train_set_features 训练集特征\n",
    "    :train_label 训练集标签\n",
    "    :val_set_features 验证集特征\n",
    "    :val_label 验证集标签\n",
    "    :test_set_features 测试集特征\n",
    "    :test_label 测试集标签\n",
    "    :n_est 构建多少棵树，默认200棵\n",
    "    '''\n",
    "    from sklearn.ensemble import RandomForestClassifier \n",
    "    from sklearn.metrics import classification_report\n",
    "    \n",
    "    rf_model = RandomForestClassifier(n_estimators=n_est, random_state=1080, n_jobs=-1) \n",
    "    rf_model.fit(train_set_features, train_label) \n",
    "    print \"val mean accuracy: {0}\".format(rf_model.score(val_set_features, val_label)) \n",
    "    y_pred = rf_model.predict(test_set_features) \n",
    "    print classification_report(test_label, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 特征提取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "how many words: 179774\n",
      "tf-idf shape: (220000,179774)\n",
      "vectors shape: (220000,179774)\n",
      "CPU times: user 27 s, sys: 567 ms, total: 27.5 s\n",
      "Wall time: 27.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "## 从磁盘中读取样本标签和语料库\n",
    "# 训练集，65000, 验证集，6500, 测试集，16578\n",
    "# train_label, train_corpus, val_label, val_corpus, test_label, test_corpus = get_samples_from_file_6500('/home/beanyon/Desktop/logistic_regression/corpus.txt')\n",
    "train_label, train_corpus, val_label, val_corpus, test_label, test_corpus = get_samples_from_file_20000('/home/beanyon/Desktop/preprogress/corpus.txt')\n",
    "# 将语料库合并，用于计算TF-IDF值\n",
    "all_corpus = get_all_corpus(train_corpus, val_corpus, test_corpus)\n",
    "# 全量计算TF-IDF值\n",
    "all_features = calc_tf_idf(all_corpus)\n",
    "# 分割样本集为训练集、验证集、测试集\n",
    "# train_features_set = all_features[:65000]\n",
    "# val_features_set = all_features[65000:71500]\n",
    "# test_features_set = all_features[71500:]\n",
    "train_features_set = all_features[:165000]\n",
    "val_features_set = all_features[165000:176000]\n",
    "test_features_set = all_features[176000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用逻辑回归进行分类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val mean accuracy: 0.951272727273\n"
     ]
    },
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'ascii' codec can't decode byte 0xe4 in position 0: ordinal not in range(128)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-e34d88da6b65>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mu'time'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mu''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mu'# \\u4f7f\\u7528\\u903b\\u8f91\\u56de\\u5f52\\u8fdb\\u884c\\u5206\\u7c7b\\nlogistic_classifier(train_features_set, train_label, \\n                    val_features_set, val_label, \\n                    test_features_set, test_label)'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/beanyon/anaconda2/lib/python2.7/site-packages/IPython/core/interactiveshell.pyc\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2115\u001b[0m             \u001b[0mmagic_arg_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar_expand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstack_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2116\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2117\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2118\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<decorator-gen-60>\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n",
      "\u001b[0;32m/home/beanyon/anaconda2/lib/python2.7/site-packages/IPython/core/magic.pyc\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/beanyon/anaconda2/lib/python2.7/site-packages/IPython/core/magics/execution.pyc\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[1;32m   1187\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m'eval'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1188\u001b[0m             \u001b[0mst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1189\u001b[0;31m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_ns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1190\u001b[0m             \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1191\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<timed eval>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-172b6fc2abb6>\u001b[0m in \u001b[0;36mlogistic_classifier\u001b[0;34m(train_set_features, train_label, val_set_features, val_label, test_set_features, test_label)\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32mprint\u001b[0m \u001b[0;34m\"val mean accuracy: {0}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlr_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_set_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_label\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlr_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_set_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0;32mprint\u001b[0m \u001b[0mclassification_report\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_label\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/beanyon/.local/lib/python2.7/site-packages/sklearn/metrics/classification.pyc\u001b[0m in \u001b[0;36mclassification_report\u001b[0;34m(y_true, y_pred, labels, target_names, sample_weight, digits)\u001b[0m\n\u001b[1;32m   1432\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1433\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtarget_names\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1434\u001b[0;31m         \u001b[0mtarget_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34mu'%s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1435\u001b[0m     \u001b[0mname_width\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcn\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtarget_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1436\u001b[0m     \u001b[0mwidth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_width\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlast_line_heading\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdigits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m: 'ascii' codec can't decode byte 0xe4 in position 0: ordinal not in range(128)"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# 使用逻辑回归进行分类\n",
    "logistic_classifier(train_features_set, train_label, \n",
    "                    val_features_set, val_label, \n",
    "                    test_features_set, test_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用随机森林进行分类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/beanyon/.local/lib/python2.7/site-packages/sklearn/ensemble/weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val mean accuracy: 0.942545454545\n"
     ]
    },
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'ascii' codec can't decode byte 0xe4 in position 0: ordinal not in range(128)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-df62e42bb509>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mu'time'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mu''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mu'# \\u4f7f\\u7528\\u968f\\u673a\\u68ee\\u6797\\u8fdb\\u884c\\u5206\\u7c7b\\nrandom_forest_classifier(train_features_set, train_label,\\n                         val_features_set, val_label,\\n                         test_features_set, test_label,\\n                         n_est=200)'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/beanyon/anaconda2/lib/python2.7/site-packages/IPython/core/interactiveshell.pyc\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2115\u001b[0m             \u001b[0mmagic_arg_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar_expand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstack_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2116\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2117\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2118\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<decorator-gen-60>\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n",
      "\u001b[0;32m/home/beanyon/anaconda2/lib/python2.7/site-packages/IPython/core/magic.pyc\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/beanyon/anaconda2/lib/python2.7/site-packages/IPython/core/magics/execution.pyc\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[1;32m   1187\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m'eval'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1188\u001b[0m             \u001b[0mst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1189\u001b[0;31m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_ns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1190\u001b[0m             \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1191\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<timed eval>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-e6782fe08d0f>\u001b[0m in \u001b[0;36mrandom_forest_classifier\u001b[0;34m(train_set_features, train_label, val_set_features, val_label, test_set_features, test_label, n_est)\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;32mprint\u001b[0m \u001b[0;34m\"val mean accuracy: {0}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrf_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_set_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_label\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrf_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_set_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0;32mprint\u001b[0m \u001b[0mclassification_report\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_label\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/beanyon/.local/lib/python2.7/site-packages/sklearn/metrics/classification.pyc\u001b[0m in \u001b[0;36mclassification_report\u001b[0;34m(y_true, y_pred, labels, target_names, sample_weight, digits)\u001b[0m\n\u001b[1;32m   1432\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1433\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtarget_names\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1434\u001b[0;31m         \u001b[0mtarget_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34mu'%s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1435\u001b[0m     \u001b[0mname_width\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcn\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtarget_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1436\u001b[0m     \u001b[0mwidth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_width\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlast_line_heading\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdigits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m: 'ascii' codec can't decode byte 0xe4 in position 0: ordinal not in range(128)"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# 使用随机森林进行分类\n",
    "random_forest_classifier(train_features_set, train_label,\n",
    "                         val_features_set, val_label,\n",
    "                         test_features_set, test_label,\n",
    "                         n_est=200)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
